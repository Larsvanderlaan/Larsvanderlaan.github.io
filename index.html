<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1YWH4QXRJ8"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1YWH4QXRJ8");</script> <meta name="google-site-verification" content="Ip6s3Cb03zlI5Z5y253zrzsIfAYNA_hcRVVvk8YTLcs"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Lars van der Laan</title> <meta name="author" content="Lars van der Laan"> <meta name="description" content="I am a PhD student in Statistics at the University of Washington, Seattle. My research interests include causal inference, semiparametric statistics, debiased machine learning, statistical learning and calibration, and inference after data-driven model-selection. "> <meta name="keywords" content="debiased machine learning, causal inference, semiparametric statistics, statistical learning and calibration, selective inference after model-selection"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://larsvanderlaan.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6C%76%64%6C%61%61%6E@%75%77.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=0bwP0i4AAAAJ&amp;hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Larsvanderlaan" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/lars-van-der-laan-32367615b/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/Larsvanderlaan3" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/"> publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/cv/"> CV <span class="sr-only">(current)</span> </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown-3" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> submenus <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown-3"> <a class="dropdown-item" href="/publications/">publications</a><div class="dropdown-divider"></div> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Lars</span> van der Laan </h1> <p class="desc"><a href="#">University of Washington, Seattle</a>. Department of Statistics</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic1-1400.webp"></source> <img src="/assets/img/prof_pic1.jpg?3085aba021cc096187767b9c017ef98e" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="prof_pic1.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>B313</p> <p>Padelford Hall, Northeast Stevens Way</p> <p>Seattle, WA 98103</p> <p>lvdlaan@uw.edu</p> </div> </div> <div class="clearfix"> <h2 id="about-me">About me</h2> <p>I am a final year Ph.D. student in Statistics at the University of Washington, advised by <a href="http://faculty.washington.edu/mcarone/about.html" rel="external nofollow noopener" target="_blank">Marco Carone</a> and <a href="http://www.alexluedtke.com" rel="external nofollow noopener" target="_blank">Alex Luedtke</a>.</p> <p>My research focuses on <strong>causal inference, statistical learning, and semiparametric efficiency theory</strong>. I develop methods for debiased and efficient estimation with modern machine learning, including <a href="https://arxiv.org/pdf/2411.02771" rel="external nofollow noopener" target="_blank">doubly robust inference</a>, <a href="https://arxiv.org/pdf/2501.11868" rel="external nofollow noopener" target="_blank">automatic debiasing</a>, inference after <a href="https://arxiv.org/pdf/2307.12544" rel="external nofollow noopener" target="_blank">model selection</a>, and <a href="https://arxiv.org/pdf/2402.01972" rel="external nofollow noopener" target="_blank">heterogeneous treatment effects</a>.</p> <p>I am supported by a Netflix Graduate Research Fellowship, working with <a href="https://nathankallus.com/" rel="external nofollow noopener" target="_blank">Nathan Kallus</a> and <a href="https://scholar.google.com/citations?user=N_8WC5oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Aurélien Bibaut</a> on <strong>surrogate endpoints</strong> and <strong>reinforcement learning for dynamic decision making</strong>. Our projects span <strong>semiparametric reinforcement learning</strong> for long-term causal inference (<a href="https://arxiv.org/pdf/2501.06926" rel="external nofollow noopener" target="_blank">paper</a>), <strong>nonparametric instrumental variables inference</strong> (<a href="https://arxiv.org/pdf/2505.07729" rel="external nofollow noopener" target="_blank">paper</a>), and <strong>inverse reinforcement learning</strong> (<a href="https://arxiv.org/pdf/2509.21172" rel="external nofollow noopener" target="_blank">paper</a>).</p> <p>Another line of my research leverages <strong>calibration</strong>, a tool from machine learning, to advance methods in <strong>causal inference and dynamic decision-making</strong>. Examples include <a href="https://proceedings.mlr.press/v202/van-der-laan23a/van-der-laan23a.pdf" rel="external nofollow noopener" target="_blank">causal isotonic calibration</a> for CATE estimation, <a href="https://arxiv.org/pdf/2411.06342" rel="external nofollow noopener" target="_blank">stabilized inverse probability weighting</a> for robust weighting, <a href="https://arxiv.org/pdf/2501.06926" rel="external nofollow noopener" target="_blank">Bellman calibration</a> for reinforcement learning, and <a href="https://arxiv.org/pdf/2411.02771" rel="external nofollow noopener" target="_blank">doubly robust inference via calibration</a> for debiased estimation and valid confidence intervals under slow or inconsistent nuisance convergence.</p> <p>I also study <strong>calibration methods for predictive inference and machine learning</strong>, with a focus on developing rigorous uncertainty guarantees for modern predictive models. This includes <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/c1c49aba08e6c90f2b1f85751f497a2f-Paper-Conference.pdf" rel="external nofollow noopener" target="_blank">self-calibrating conformal prediction</a> and <a href="https://arxiv.org/pdf/2502.05676" rel="external nofollow noopener" target="_blank">generalized Venn–Abers calibration</a> (with <a href="https://vcresearch.berkeley.edu/faculty/ahmed-alaa" rel="external nofollow noopener" target="_blank">Ahmed Alaa</a>, UC Berkeley). These frameworks provide <strong>finite-sample calibration guarantees</strong> for black-box predictors, enabling distribution-free uncertainty quantification for tasks such as regression and quantile estimation, and for constructing prediction intervals.</p> <p>Beyond methodology, I apply these ideas in biomedical and technology domains, through research internships at Genentech, the Fred Hutchinson Cancer Center, and Netflix. I also contribute to the <a href="https://tlverse.org" rel="external nofollow noopener" target="_blank">tlverse</a> open-source software ecosystem and consult for <a href="https://www.tlrevolution.com" rel="external nofollow noopener" target="_blank">TLRevolution</a>.</p> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="vanderlaan2025nonparametricinstrumentalvariableinference" class="col-sm-8"> <div class="title">Nonparametric Instrumental Variable Inference with Many Weak Instruments</div> <div class="author"> <em>Lars van der Laan</em>, Nathan Kallus, and Aurélien Bibaut</div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2505.07729" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2505.07729"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We study inference on linear functionals in the nonparametric instrumental variable (NPIV) problem with a discretely-valued instrument under a many-weak-instruments asymptotic regime, where the number of instrument values grows with the sample size. A key motivating example is estimating long-term causal effects in a new experiment with only short-term outcomes, using past experiments to instrument for the effect of short- on long-term outcomes. Here, the assignment to a past experiment serves as the instrument: we have many past experiments but only a limited number of units in each. Since the structural function is nonparametric but constrained by only finitely many moment restrictions, point identification typically fails. To address this, we consider linear functionals of the minimum-norm solution to the moment restrictions, which is always well-defined. As the number of instrument levels grows, these functionals define an approximating sequence to a target functional, replacing point identification with a weaker asymptotic notion suited to discrete instruments. Extending the Jackknife Instrumental Variable Estimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a nonparametric estimator for solutions to linear inverse problems with many weak instruments. We construct automatic debiased machine learning estimators for linear functionals of both the structural function and its minimum-norm projection, and establish their efficiency in the many-weak-instruments regime.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="vanderlaan2025automaticdebiasedmachinelearning" class="col-sm-8"> <div class="title">Automatic Debiased Machine Learning for Smooth Functionals of Nonparametric M-Estimands</div> <div class="author"> <em>Lars van der Laan</em>, Aurelien Bibaut, Nathan Kallus, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alex Luedtke' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2501.11868" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2501.11868"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> We propose a unified framework for automatic debiased machine learning (autoDML) to perform inference on smooth functionals of infinite-dimensional M-estimands, defined as population risk minimizers over Hilbert spaces. By automating debiased estimation and inference procedures in causal inference and semiparametric statistics, our framework enables practitioners to construct valid estimators for complex parameters without requiring specialized expertise. The framework supports Neyman-orthogonal loss functions with unknown nuisance parameters requiring data-driven estimation, as well as vector-valued M-estimands involving simultaneous loss minimization across multiple Hilbert space models. We formalize the class of parameters efficiently estimable by autoDML as a novel class of nonparametric projection parameters, defined via orthogonal minimum loss objectives. We introduce three autoDML estimators based on one-step estimation, targeted minimum loss-based estimation, and the method of sieves. For data-driven model selection, we derive a novel decomposition of model approximation error for smooth functionals of M-estimands and propose adaptive debiased machine learning estimators that are superefficient and adaptive to the functional form of the M-estimand. Finally, we illustrate the flexibility of our framework by constructing autoDML estimators for the long-term survival under a beta-geometric model. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="vanderlaan2025automaticdoublereinforcementlearning" class="col-sm-8"> <div class="title">Semiparametric Double Reinforcement Learning with Applications to Long-Term Causal Inference</div> <div class="author"> <em>Lars van der Laan</em>, David Hubbard, Allen Tran, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Nathan Kallus, Aurélien Bibaut' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2501.06926" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Larsvanderlaan/Larsvanderlaan.github.io/blob/master/pdfs_papers/MarkovDRLSlides.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2501.06926"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Long-term causal effects often must be estimated from short-term data due to limited follow-up in healthcare, economics, and online platforms. Markov Decision Processes (MDPs) provide a natural framework for capturing such long-term dynamics through sequences of states, actions, and rewards. Double Reinforcement Learning (DRL) enables efficient inference on policy values in MDPs, but nonparametric implementations require strong intertemporal overlap assumptions and often exhibit high variance and instability. We propose a semiparametric extension of DRL for efficient inference on linear functionals of the Q-function–such as policy values–in infinite-horizon, time-homogeneous MDPs. By imposing structural restrictions on the Q-function, our approach relaxes the strong overlap conditions required by nonparametric methods and improves statistical efficiency. Under model misspecification, our estimators target the functional of the best-approximating Q-function, with only second-order bias. We provide conditions for valid inference using sieve methods and data-driven model selection. A central challenge in DRL is the estimation of nuisance functions, such as density ratios, which often involve difficult minimax optimization. To address this, we introduce a novel plug-in estimator based on isotonic Bellman calibration, which combines fitted Q-iteration with an isotonic regression adjustment. The estimator is debiased without requiring estimation of additional nuisance functions and reduces high-dimensional overlap assumptions to a one-dimensional condition. Bellman calibration extends isotonic calibration–widely used in prediction and classification–to the MDP setting and may be of independent interest.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="van2024stabilized" class="col-sm-8"> <div class="title">Stabilized Inverse Probability Weighting via Isotonic Calibration</div> <div class="author"> <em>Lars van der Laan</em>, Ziming Lin, Marco Carone, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alex Luedtke' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 3rd Conference on Causal Learning and Reasoning (CLeaR)</em>, 2025 </div> <div class="periodical"> To appear </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2411.06342" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://ngreifer.github.io/WeightIt/reference/calibrate.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Inverse weighting with an estimated propensity score is widely used by estimation methods in causal inference to adjust for confounding bias. However, directly inverting propensity score estimates can lead to instability, bias, and excessive variability due to large inverse weights, especially when treatment overlap is limited. In this work, we propose a post-hoc calibration algorithm for inverse propensity weights that generates well-calibrated, stabilized weights from user-supplied, cross-fitted propensity score estimates. Our approach employs a variant of isotonic regression with a loss function specifically tailored to the inverse propensity weights. Through theoretical analysis and empirical studies, we demonstrate that isotonic calibration improves the performance of doubly robust estimators of the average treatment effect.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="van2025inverse" class="col-sm-8"> <div class="title">Inverse Reinforcement Learning Using Just Classification and a Few Regressions</div> <div class="author"> Lars Laan, Nathan Kallus, and Aurélien Bibaut</div> <div class="periodical"> <em>arXiv preprint arXiv:2509.21172</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2509.21172" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Inverse reinforcement learning (IRL) aims to explain observed behavior by uncovering an underlying reward. In the maximum-entropy or Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function and a soft value function that together satisfy the soft Bellman consistency condition and maximize the likelihood of observed actions. While this perspective has had enormous impact in imitation learning for robotics and understanding dynamic choices in economics, practical learning algorithms often involve delicate inner-loop optimization, repeated dynamic programming, or adversarial training, all of which complicate the use of modern, highly expressive function approximators like neural nets and boosting. We revisit softmax IRL and show that the population maximum-likelihood solution is characterized by a linear fixed-point equation involving the behavior policy. This observation reduces IRL to two off-the-shelf supervised learning problems: probabilistic classification to estimate the behavior policy, and iterative regression to solve the fixed point. The resulting method is simple and modular across function approximation classes and algorithms. We provide a precise characterization of the optimal solution, a generic oracle-based algorithm, finite-sample error bounds, and empirical results showing competitive or superior performance to MaxEnt IRL.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="van2024automatic" class="col-sm-8"> <div class="title">Doubly robust inference via calibration</div> <div class="author"> <em>Lars van der Laan</em>, Alex Luedtke, and Marco Carone</div> <div class="periodical"> <em>arXiv preprint arXiv:2411.02771</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2411.02771" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Larsvanderlaan/CDML" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://github.com/Larsvanderlaan/Larsvanderlaan.github.io/blob/master/pdfs_papers/posterDoublyRobustCalibration.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Doubly robust estimators are widely used for estimating average treatment effects and other linear summaries of regression functions. While consistency requires only one of two nuisance functions to be estimated consistently, asymptotic normality typically require sufficiently fast convergence of both. In this work, we correct this mismatch: we show that calibrating the nuisance estimators within a doubly robust procedure yields doubly robust asymptotic normality for linear functionals. We introduce a general framework, calibrated debiased machine learning (calibrated DML), and propose a specific estimator that augments standard DML with a simple isotonic regression adjustment. Our theoretical analysis shows that the calibrated DML estimator remains asymptotically normal if either the regression or the Riesz representer of the functional is estimated sufficiently well, allowing the other to converge arbitrarily slowly or even inconsistently. We further propose a simple bootstrap method for constructing confidence intervals, enabling doubly robust inference without additional nuisance estimation. In a range of semi-synthetic benchmark datasets, calibrated DML reduces bias and improves coverage relative to standard DML. Our method can be integrated into existing DML pipelines by adding just a few lines of code to calibrate cross-fitted estimates via isotonic regression.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="vanderlaan2024selfconsistent" class="col-sm-8"> <div class="title">Self-Calibrating Conformal Prediction</div> <div class="author"> <em>Lars van der Laan</em>, and Ahmed M. Alaa</div> <div class="periodical"> <em>The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openreview.net/pdf?id=BJ6HkT7qIk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Larsvanderlaan/SelfCalibratingConformal" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202024/96201.png?t=1733961325.1491308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> In machine learning, model calibration and predictive inference are essential for producing reliable predictions and quantifying uncertainty to support decision- making. Recognizing the complementary roles of point and interval predictions, we introduce Self-Calibrating Conformal Prediction, a method that combines Venn- Abers calibration and conformal prediction to deliver calibrated point predictions alongside prediction intervals with finite-sample validity conditional on these pre- dictions. To achieve this, we extend the original Venn-Abers procedure from binary classification to regression. Our theoretical framework supports analyzing confor- mal prediction methods that involve calibrating model predictions and subsequently constructing conditionally valid prediction intervals on the same data, where the conditioning set or conformity scores may depend on the calibrated predictions. Real-data experiments show that our method improves interval efficiency through model calibration and offers a practical alternative to feature-conditional validity. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="van2024combining" class="col-sm-8"> <div class="title">Combining T-learning and DR-learning: a framework for oracle-efficient estimation of causal contrasts</div> <div class="author"> <em>Lars van der Laan</em>, Marco Carone, and Alex Luedtke</div> <div class="periodical"> <em>arXiv preprint arXiv:2402.01972</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2402.01972.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Larsvanderlaan/hte3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://pbs.twimg.com/media/GN4OvShakAABC4T?format=jpg&amp;name=4096x4096" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> We introduce efficient plug-in (EP) learning, a novel framework for the estimation of heterogeneous causal contrasts, such as the conditional average treatment effect and conditional relative risk. The EP-learning framework enjoys the same oracle-efficiency as Neyman-orthogonal learning strategies, such as DR-learning and R-learning, while addressing some of their primary drawbacks, including that (i) their practical applicability can be hindered by loss function non-convexity; and (ii) they may suffer from poor performance and instability due to inverse probability weighting and pseudo-outcomes that violate bounds. To avoid these drawbacks, EP-learner constructs an efficient plug-in estimator of the population risk function for the causal contrast, thereby inheriting the stability and robustness properties of plug-in estimation strategies like T-learning. Under reasonable conditions, EP-learners based on empirical risk minimization are oracle-efficient, exhibiting asymptotic equivalence to the minimizer of an oracle-efficient one-step debiased estimator of the population risk function. In simulation experiments, we illustrate that EP-learners of the conditional average treatment effect and conditional relative risk outperform state-of-the-art competitors, including T-learner, R-learner, and DR-learner. Open-source implementations of the proposed methods are available in our R package hte3. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="van2023causal" class="col-sm-8"> <div class="title">Causal isotonic calibration for heterogeneous treatment effects</div> <div class="author"> <em>Lars van der Laan</em>, Ernesto Ulloa-Pérez, Marco Carone, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Alex Luedtke' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 40th International Conference on Machine Learning (ICML)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.linkedin.com/feed/update/urn:li:activity:7075389038043533312?utm_source=share&amp;utm_medium=member_desktop" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://proceedings.mlr.press/v202/van-der-laan23a/van-der-laan23a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://twitter.com/LarsvanderLaan3/status/1630996161267122177" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Twitter</a> <a href="https://github.com/Larsvanderlaan/causalCalibration" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://icml.cc/media/PosterPDFs/ICML%202023/24460.png?t=1690505587.8818903" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We propose causal isotonic calibration, a novel nonparametric method for calibrating predictors of heterogeneous treatment effects. Furthermore, we introduce cross-calibration, a data-efficient variant of calibration that eliminates the need for hold-out calibration sets. Cross-calibration leverages cross-fitted predictors and generates a single calibrated predictor using all available data. Under weak conditions that do not assume monotonicity, we establish that both causal isotonic calibration and cross-calibration achieve fast doubly-robust calibration rates, as long as either the propensity score or outcome regression is estimated accurately in a suitable sense. The proposed causal isotonic calibrator can be wrapped around any black-box learning algorithm, providing robust and distribution-free calibration guarantees while preserving predictive performance.</p> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6C%76%64%6C%61%61%6E@%75%77.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=0bwP0i4AAAAJ&amp;hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Larsvanderlaan" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/lars-van-der-laan-32367615b/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/Larsvanderlaan3" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> To contact me, email me at lvdlaan@uw.edu or connect with me on LinkedIn </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Lars van der Laan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1YWH4QXRJ8"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1YWH4QXRJ8");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>