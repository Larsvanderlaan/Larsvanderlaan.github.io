---
layout: about
title: about
permalink: /
subtitle: <a href='#'>University of Washington, Seattle</a>. Department of Statistics

profile:
  align: right
  image: prof_pic1.jpg 
  image_circular: true # crops the image to make it circular
  address: >
    <p>B313</p>
    <p>Padelford Hall, Northeast Stevens Way</p>
    <p>Seattle, WA 98103</p>
    <p>lvdlaan@uw.edu</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---


## About me  

I am a final-year Ph.D. candidate in Statistics at the University of Washington, advised by [Marco Carone](http://faculty.washington.edu/mcarone/about.html) and [Alex Luedtke](http://www.alexluedtke.com).  

My research focuses on **causal inference, semiparametric statistics, and reinforcement learning**. I develop methods for debiased and efficient estimation with machine learning, including [doubly robust inference](https://arxiv.org/pdf/2411.02771), [automatic debiasing](https://arxiv.org/pdf/2501.11868), inference after [model selection](https://arxiv.org/pdf/2307.12544), and [heterogeneous treatment effects](https://arxiv.org/pdf/2402.01972).

I am supported by a Netflix Graduate Research Fellowship and work closely with [Nathan Kallus](https://nathankallus.com/) and [Aurélien Bibaut](https://scholar.google.com/citations?user=N_8WC5oAAAAJ&hl=en). My projects span [long-term causal inference](https://arxiv.org/pdf/2501.06926) via reinforcement learning, nonparametric [instrumental variables](https://arxiv.org/pdf/2505.07729) inference, and inverse reinforcement learning [estimation](https://arxiv.org/pdf/2509.21172) and [inference](https://arxiv.org/pdf/2512.24407). I also work on RL theory for value estimation via [Fitted Q Evaluation](https://arxiv.org/pdf/2512.23805) and [Fitted Q Iteration](https://arxiv.org/pdf/2512.23927).

Another line of my research adapts **calibration**—a post-hoc technique from predictive modeling—to advance methods in **causal inference and dynamic decision-making**. Examples include [causal isotonic calibration](https://proceedings.mlr.press/v202/van-der-laan23a/van-der-laan23a.pdf) for conditional average treatment effects, calibration-based stabilization  of [inverse probability weighting](https://arxiv.org/pdf/2411.06342) estimators, [Bellman calibration](https://arxiv.org/pdf/2512.23694) for offline reinforcement learning, and [calibrated debiased machine learning](https://arxiv.org/pdf/2411.02771) for doubly robust inference under slow or inconsistent nuisance estimation.


I also develop **methods for predictive uncertainty quantification**, focusing on **finite-sample, distribution-free guarantees** for black-box models. This includes [self-calibrating conformal prediction](https://proceedings.neurips.cc/paper_files/paper/2024/file/c1c49aba08e6c90f2b1f85751f497a2f-Paper-Conference.pdf) and [generalized Venn–Abers calibration](https://arxiv.org/pdf/2502.05676) (with [Ahmed Alaa](https://vcresearch.berkeley.edu/faculty/ahmed-alaa), UC Berkeley), with applications to regression, quantile estimation, and prediction intervals.

Beyond methodology, I apply these ideas in biomedical and technology domains through research internships at Genentech, the Fred Hutchinson Cancer Center, and Netflix. I also contribute to the [tlverse](https://tlverse.org) open-source software ecosystem and consult for [TLRevolution](https://www.tlrevolution.com).
