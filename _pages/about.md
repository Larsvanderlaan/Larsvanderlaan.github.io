---
layout: about
title: about
permalink: /
subtitle: <a href='#'>University of Washington, Seattle</a>. Department of Statistics

profile:
  align: right
  image: prof_pic1.jpg 
  image_circular: true # crops the image to make it circular
  address: >
    <p>B313</p>
    <p>Padelford Hall, Northeast Stevens Way</p>
    <p>Seattle, WA 98103</p>
    <p>lvdlaan@uw.edu</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

## About me

I am a final year Ph.D. student in Statistics at the University of Washington, advised by [Marco Carone](http://faculty.washington.edu/mcarone/about.html) and [Alex Luedtke](http://www.alexluedtke.com).

My research focuses on **causal inference, statistical learning, and semiparametric efficiency theory**. I develop methods for debiased and efficient estimation with modern machine learning, including [doubly robust inference](https://arxiv.org/pdf/2411.02771), [automatic debiasing](https://arxiv.org/pdf/2501.11868), inference after [model selection](https://arxiv.org/pdf/2307.12544), and [heterogeneous treatment effects](https://arxiv.org/pdf/2402.01972).

I am supported by a Netflix Graduate Research Fellowship, working with [Nathan Kallus](https://nathankallus.com/) and [Aurélien Bibaut](https://scholar.google.com/citations?user=N_8WC5oAAAAJ&hl=en) on **surrogate endpoints** and **reinforcement learning for dynamic decision making**. Our projects span reinforcement learning for long-term causal inference ([paper](https://arxiv.org/pdf/2501.06926)), nonparametric instrumental variables inference ([paper](https://arxiv.org/pdf/2505.07729)), and inverse reinforcement learning ([paper](https://arxiv.org/pdf/2509.21172)).

Another line of my research leverages **calibration**, a tool from machine learning, to advance methods in **causal inference and dynamic decision-making**. Examples include [causal isotonic calibration](https://proceedings.mlr.press/v202/van-der-laan23a/van-der-laan23a.pdf) for CATE estimation, [stabilized inverse probability weighting](https://arxiv.org/pdf/2411.06342) for robust weighting, [Bellman calibration](https://arxiv.org/pdf/2501.06926) for reinforcement learning, and [doubly robust inference via calibration](https://arxiv.org/pdf/2411.02771) for debiased estimation and valid confidence intervals under slow or inconsistent nuisance convergence.

I also study **calibration methods for predictive inference and machine learning**, with a focus on developing rigorous uncertainty guarantees for modern predictive models. This includes [self-calibrating conformal prediction](https://proceedings.neurips.cc/paper_files/paper/2024/file/c1c49aba08e6c90f2b1f85751f497a2f-Paper-Conference.pdf) and [generalized Venn–Abers calibration](https://arxiv.org/pdf/2502.05676) (with [Ahmed Alaa](https://vcresearch.berkeley.edu/faculty/ahmed-alaa), UC Berkeley). These frameworks provide **finite-sample calibration guarantees** for black-box predictors, enabling distribution-free uncertainty quantification for tasks such as regression and quantile estimation, and for constructing prediction intervals.



Beyond methodology, I apply these ideas in biomedical and technology domains, through research internships at Genentech, the Fred Hutchinson Cancer Center, and Netflix. I also contribute to the [tlverse](https://tlverse.org) open-source software ecosystem and consult for [TLRevolution](https://www.tlrevolution.com).
