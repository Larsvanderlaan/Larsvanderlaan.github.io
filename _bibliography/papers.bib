---

@string{aps = {American Physical Society,}}

@misc{vanderlaan2025nonparametricinstrumentalvariableinference,
      title={Nonparametric Instrumental Variable Inference with Many Weak Instruments}, 
      author={Lars {van der Laan} and Nathan Kallus and Aurélien Bibaut},
      year={2025},
      eprint={2505.07729},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      pdf={https://arxiv.org/abs/2505.07729}, 
      abstract={We study inference on linear functionals in the nonparametric instrumental variable (NPIV) problem with a discretely-valued instrument under a many-weak-instruments asymptotic regime, where the number of instrument values grows with the sample size. A key motivating example is estimating long-term causal effects in a new experiment with only short-term outcomes, using past experiments to instrument for the effect of short- on long-term outcomes. Here, the assignment to a past experiment serves as the instrument: we have many past experiments but only a limited number of units in each. Since the structural function is nonparametric but constrained by only finitely many moment restrictions, point identification typically fails. To address this, we consider linear functionals of the minimum-norm solution to the moment restrictions, which is always well-defined. As the number of instrument levels grows, these functionals define an approximating sequence to a target functional, replacing point identification with a weaker asymptotic notion suited to discrete instruments. Extending the Jackknife Instrumental Variable Estimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a nonparametric estimator for solutions to linear inverse problems with many weak instruments. We construct automatic debiased machine learning estimators for linear functionals of both the structural function and its minimum-norm projection, and establish their efficiency in the many-weak-instruments regime.},
      keywords={effest},
      selected={true}
}


@inproceedings{van2025generalized,
  title={Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction},
  author={{van der Laan}, Lars and Alaa, Ahmed},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year={2025},
  note={To appear},
  abstract={Ensuring model calibration is critical for reliable predictions, yet popular distribution-free methods, such as histogram binning and isotonic regression, provide only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration, generalizing Vovk's binary classification approach to arbitrary prediction tasks and loss functions. Venn calibration leverages binning calibrators to construct prediction sets that contain at least one marginally perfectly calibrated point prediction in finite samples, capturing epistemic uncertainty in the calibration process. The width of these sets shrinks asymptotically to zero, converging to a conditionally calibrated point prediction. Furthermore, we propose Venn multicalibration, a novel methodology for finite-sample calibration across subpopulations. For quantile loss, group-conditional and multicalibrated conformal prediction arise as special cases of Venn multicalibration, and Venn calibration produces novel conformal prediction intervals that achieve quantile-conditional coverage. As a separate contribution, we extend distribution-free conditional calibration guarantees of histogram binning and isotonic calibration to general losses.},
  keywords={calibration},
  pdf={https://arxiv.org/pdf/2502.05676}
}


@misc{vanderlaan2025automaticdebiasedmachinelearning,
      title={Automatic Debiased Machine Learning for Smooth Functionals of Nonparametric M-Estimands}, 
      author={Lars {van der Laan} and Aurelien Bibaut and Nathan Kallus and Alex Luedtke},
      year={2025},
      eprint={2501.11868},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2501.11868}, 
      keywords={effest},
      selected={true},
	pdf={https://arxiv.org/abs/2501.11868},
abstract={
We propose a unified framework for automatic debiased machine learning (autoDML) to perform inference on smooth functionals of infinite-dimensional M-estimands, defined as population risk minimizers over Hilbert spaces. By automating debiased estimation and inference procedures in causal inference and semiparametric statistics, our framework enables practitioners to construct valid estimators for complex parameters without requiring specialized expertise. The framework supports Neyman-orthogonal loss functions with unknown nuisance parameters requiring data-driven estimation, as well as vector-valued M-estimands involving simultaneous loss minimization across multiple Hilbert space models. We formalize the class of parameters efficiently estimable by autoDML as a novel class of nonparametric projection parameters, defined via orthogonal minimum loss objectives. We introduce three autoDML estimators based on one-step estimation, targeted minimum loss-based estimation, and the method of sieves. For data-driven model selection, we derive a novel decomposition of model approximation error for smooth functionals of M-estimands and propose adaptive debiased machine learning estimators that are superefficient and adaptive to the functional form of the M-estimand. Finally, we illustrate the flexibility of our framework by constructing autoDML estimators for the long-term survival under a beta-geometric model.
}
}


@misc{vanderlaan2025automaticdoublereinforcementlearning,
      title={Semiparametric Double Reinforcement Learning with Applications to Long-Term Causal Inference}, 
      author={{van der Laan}, Lars and Hubbard, David and Tran, Allen and Kallus, Nathan and Bibaut, Aurélien},
      year={2025},
      eprint={2501.06926},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2501.06926}, 
      keywords={effest},
      selected={true},
      abstract={Long-term causal effects often must be estimated from short-term data due to limited follow-up in healthcare, economics, and online platforms. Markov Decision Processes (MDPs) provide a natural framework for capturing such long-term dynamics through sequences of states, actions, and rewards. Double Reinforcement Learning (DRL) enables efficient inference on policy values in MDPs, but nonparametric implementations require strong intertemporal overlap assumptions and often exhibit high variance and instability. We propose a semiparametric extension of DRL for efficient inference on linear functionals of the Q-function--such as policy values--in infinite-horizon, time-homogeneous MDPs. By imposing structural restrictions on the Q-function, our approach relaxes the strong overlap conditions required by nonparametric methods and improves statistical efficiency. Under model misspecification, our estimators target the functional of the best-approximating Q-function, with only second-order bias. We provide conditions for valid inference using sieve methods and data-driven model selection. A central challenge in DRL is the estimation of nuisance functions, such as density ratios, which often involve difficult minimax optimization. To address this, we introduce a novel plug-in estimator based on isotonic Bellman calibration, which combines fitted Q-iteration with an isotonic regression adjustment. The estimator is debiased without requiring estimation of additional nuisance functions and reduces high-dimensional overlap assumptions to a one-dimensional condition. Bellman calibration extends isotonic calibration--widely used in prediction and classification--to the MDP setting and may be of independent interest.},
pdf={https://arxiv.org/pdf/2501.06926},
poster={https://github.com/Larsvanderlaan/Larsvanderlaan.github.io/blob/master/pdfs_papers/MarkovDRLSlides.pdf}
}



@article{van2024automatic,
   selected={true},
  title={Doubly robust inference via calibration},
   abstract={Doubly robust estimators are widely used for estimating average treatment effects and other linear summaries of regression functions. While consistency requires only one of two nuisance functions to be estimated consistently, asymptotic normality typically require sufficiently fast convergence of both. In this work, we correct this mismatch: we show that calibrating the nuisance estimators within a doubly robust procedure yields doubly robust asymptotic normality for linear functionals. We introduce a general framework, calibrated debiased machine learning (calibrated DML), and propose a specific estimator that augments standard DML with a simple isotonic regression adjustment. Our theoretical analysis shows that the calibrated DML estimator remains asymptotically normal if either the regression or the Riesz representer of the functional is estimated sufficiently well, allowing the other to converge arbitrarily slowly or even inconsistently. We further propose a simple bootstrap method for constructing confidence intervals, enabling doubly robust inference without additional nuisance estimation. In a range of semi-synthetic benchmark datasets, calibrated DML reduces bias and improves coverage relative to standard DML. Our method can be integrated into existing DML pipelines by adding just a few lines of code to calibrate cross-fitted estimates via isotonic regression.},
  author={{van der Laan}, Lars and Luedtke, Alex and Carone, Marco},
  journal={arXiv preprint arXiv:2411.02771},
  pdf={https://arxiv.org/pdf/2411.02771},
  code={https://github.com/Larsvanderlaan/CDML},
  poster={https://github.com/Larsvanderlaan/Larsvanderlaan.github.io/blob/master/pdfs_papers/posterDoublyRobustCalibration.pdf},
  keywords={causalcal},
  year={2024}
}


@inproceedings{van2024stabilized,
  title={Stabilized Inverse Probability Weighting via Isotonic Calibration},
  author={{van der Laan}, Lars and Lin, Ziming and Carone, Marco and Luedtke, Alex},
  booktitle={Proceedings of the 3rd Conference on Causal Learning and Reasoning (CLeaR)},
  year={2025},
  note={To appear},
  abstract={Inverse weighting with an estimated propensity score is widely used by estimation methods in causal inference to adjust for confounding bias. However, directly inverting propensity score estimates can lead to instability, bias, and excessive variability due to large inverse weights, especially when treatment overlap is limited. In this work, we propose a post-hoc calibration algorithm for inverse propensity weights that generates well-calibrated, stabilized weights from user-supplied, cross-fitted propensity score estimates. Our approach employs a variant of isotonic regression with a loss function specifically tailored to the inverse propensity weights. Through theoretical analysis and empirical studies, we demonstrate that isotonic calibration improves the performance of doubly robust estimators of the average treatment effect.},
  pdf={https://arxiv.org/pdf/2411.06342},
  code={https://ngreifer.github.io/WeightIt/reference/calibrate.html},
  keywords={causalcal},
  selected={true}
}




@article{vanderlaan2024selfconsistent,
      title={Self-Calibrating Conformal Prediction},
      author={{van der Laan}, Lars and {M. Alaa}, Ahmed},
      year={2024},
        journal={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
          selected={true},
          abstract={
          In machine learning, model calibration and predictive inference are essential for producing reliable predictions and quantifying uncertainty to support decision- making. Recognizing the complementary roles of point and interval predictions, we introduce Self-Calibrating Conformal Prediction, a method that combines Venn- Abers calibration and conformal prediction to deliver calibrated point predictions alongside prediction intervals with finite-sample validity conditional on these pre- dictions. To achieve this, we extend the original Venn-Abers procedure from binary classification to regression. Our theoretical framework supports analyzing confor- mal prediction methods that involve calibrating model predictions and subsequently constructing conditionally valid prediction intervals on the same data, where the conditioning set or conformity scores may depend on the calibrated predictions. Real-data experiments show that our method improves interval efficiency through model calibration and offers a practical alternative to feature-conditional validity.
},
keywords={calibration},
code={https://github.com/Larsvanderlaan/SelfCalibratingConformal},
 poster={https://neurips.cc/media/PosterPDFs/NeurIPS%202024/96201.png?t=1733961325.1491308},
pdf={https://openreview.net/pdf?id=BJ6HkT7qIk}
}



@article{van2024adaptive,
  title={Adaptive-TMLE for the Average Treatment Effect based on Randomized Controlled Trial Augmented with Real-World Data},
  author={{van der Laan}, Mark and Qiu, Sky and {van der Laan}, Lars},
  journal={arXiv preprint arXiv:2405.07186},
  year={2024},
  keywords={methods},
	selected={false},
          abstract={
          We consider the problem of estimating the average treatment effect (ATE) when both randomized control trial (RCT) data and real-world data (RWD) are available. We decompose the ATE estimand as the difference between a pooled-ATE estimand that integrates RCT and RWD and a bias estimand that captures the conditional effect of RCT enrollment on the outcome. We introduce an adaptive targeted minimum loss-based estimation (A-TMLE) framework to estimate them. We prove that the A-TMLE estimator is root-n-consistent and asymptotically normal. Moreover, in finite sample, it achieves the super-efficiency one would obtain had one known the oracle model for the conditional effect of the RCT enrollment on the outcome. Consequently, the smaller the working model of the bias induced by the RWD is, the greater our estimator's efficiency, while our estimator will always be at least as efficient as an efficient estimator that uses the RCT data only. A-TMLE outperforms existing methods in simulations by having smaller mean-squared-error and 95% confidence intervals. A-TMLE could help utilize RWD to improve the efficiency of randomized trial results without biasing the estimates of intervention effects. This approach could allow for smaller, faster trials, decreasing the time until patients can receive effective treatments.
},
pdf={https://arxiv.org/pdf/2405.07186}
}




@article{van2024combining,
  title={Combining T-learning and DR-learning: a framework for oracle-efficient estimation of causal contrasts},
  author={{van der Laan}, Lars and Carone, Marco and Luedtke, Alex},
  journal={arXiv preprint arXiv:2402.01972},
  year={2024},
    selected={true},
    pdf={https://arxiv.org/pdf/2402.01972.pdf},
    code={https://github.com/Larsvanderlaan/hte3},
    poster={https://pbs.twimg.com/media/GN4OvShakAABC4T?format=jpg&name=4096x4096},
      keywords={effest},
    abstract={
    We introduce efficient plug-in (EP) learning, a novel framework for the estimation of heterogeneous causal contrasts, such as the conditional average treatment effect and conditional relative risk. The EP-learning framework enjoys the same oracle-efficiency as Neyman-orthogonal learning strategies, such as DR-learning and R-learning, while addressing some of their primary drawbacks, including that (i) their practical applicability can be hindered by loss function non-convexity; and (ii) they may suffer from poor performance and instability due to inverse probability weighting and pseudo-outcomes that violate bounds. To avoid these drawbacks, EP-learner constructs an efficient plug-in estimator of the population risk function for the causal contrast, thereby inheriting the stability and robustness properties of plug-in estimation strategies like T-learning. Under reasonable conditions, EP-learners based on empirical risk minimization are oracle-efficient, exhibiting asymptotic equivalence to the minimizer of an oracle-efficient one-step debiased estimator of the population risk function. In simulation experiments, we illustrate that EP-learners of the conditional average treatment effect and conditional relative risk outperform state-of-the-art competitors, including T-learner, R-learner, and DR-learner. Open-source implementations of the proposed methods are available in our R package hte3.
}
}

@article{dutta2023estimating,
  title={Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data},
  author={Dutta, Shiladitya and Wei, Hongbo and {van der Laan}, Lars and Alaa, {Ahmed M}},
  journal={arXiv preprint arXiv:2310.09926},
  year={2023},
  pdf={https://arxiv.org/pdf/2310.09926.pdf},
    keywords={calibration},
  abstract={
  Foundation models are trained on vast amounts of data at scale using self-supervised learning, enabling adaptation to a wide range of downstream tasks. At test time, these models exhibit zero-shot capabilities through which they can classify previously unseen (user-specified) categories. In this paper, we address the problem of quantifying uncertainty in these zero-shot predictions. We propose a heuristic approach for uncertainty estimation in zero-shot settings using conformal prediction with web data. Given a set of classes at test time, we conduct zero-shot classification with CLIP-style models using a prompt template, e.g., "an image of a ", and use the same template as a search query to source calibration data from the open web. Given a web-based calibration set, we apply conformal prediction with a novel conformity score that accounts for potential errors in retrieved web data. We evaluate the utility of our proposed method in Biomedical foundation models; our preliminary results show that web-based conformal prediction sets achieve the target coverage with satisfactory efficiency on a variety of biomedical datasets.
}
}


@article{van2023adaptive,
  title={Adaptive debiased machine learning using data-driven model selection techniques},
  author={{van der Laan}, Lars and Carone, Marco and Luedtke, Alex and {van der Laan}, Mark},
  journal={arXiv preprint arXiv:2307.12544},
  year={2023},
  selected={false},
  keywords={effest},
  pdf={https://arxiv.org/pdf/2307.12544},
  abstract={Debiased machine learning estimators for nonparametric inference of smooth functionals of the data-generating distribution can suffer from excessive variability and instability. For this reason, practitioners may resort to simpler models based on parametric or semiparametric assumptions. However, such simplifying assumptions may fail to hold, and estimates may then be biased due to model misspecification. To address this problem, we propose Adaptive Debiased Machine Learning (ADML), a nonparametric framework that combines data-driven model selection and debiased machine learning techniques to construct asymptotically linear, adaptive, and superefficient estimators for pathwise differentiable functionals. By learning model structure directly from data, ADML avoids the bias introduced by model misspecification and remains free from the restrictions of parametric and semiparametric models. While they may exhibit irregular behavior for the target parameter in a nonparametric statistical model, we demonstrate that ADML estimators provides regular and locally uniformly valid inference for a projection-based oracle parameter. Importantly, this oracle parameter agrees with the original target parameter for distributions within an unknown but correctly specified oracle statistical submodel that is learned from the data. This finding implies that there is no penalty, in a local asymptotic sense, for conducting data-driven model selection compared to having prior knowledge of the oracle submodel and oracle parameter. To demonstrate the practical applicability of our theory, we provide a broad class of ADML estimators for estimating the average treatment effect in adaptive partially linear regression models.},
  code={https://github.com/Larsvanderlaan/causalHAL},
  twitter={https://twitter.com/LarsvanderLaan3/status/1683849130119151617}
}

@inproceedings{van2023causal,
  title={Causal isotonic calibration for heterogeneous treatment effects},
  author={{van der Laan}, Lars and Ulloa-P{\'e}rez, Ernesto and Carone, Marco and Luedtke, Alex},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  year={2023},
  address={Honolulu, Hawaii, USA},
  publisher={PMLR},
  volume={202},
    keywords={causalcal},
    pdf={https://proceedings.mlr.press/v202/van-der-laan23a/van-der-laan23a.pdf},
  poster={https://icml.cc/media/PosterPDFs/ICML%202023/24460.png?t=1690505587.8818903},
  abstract={We propose causal isotonic calibration, a novel nonparametric method for calibrating predictors of heterogeneous treatment effects. Furthermore, we introduce cross-calibration, a data-efficient variant of calibration that eliminates the need for hold-out calibration sets. Cross-calibration leverages cross-fitted predictors and generates a single calibrated predictor using all available data. Under weak conditions that do not assume monotonicity, we establish that both causal isotonic calibration and cross-calibration achieve fast doubly-robust calibration rates, as long as either the propensity score or outcome regression is estimated accurately in a suitable sense. The proposed causal isotonic calibrator can be wrapped around any black-box learning algorithm, providing robust and distribution-free calibration guarantees while preserving predictive performance.},
  code={https://github.com/Larsvanderlaan/causalCalibration},
  twitter={https://twitter.com/LarsvanderLaan3/status/1630996161267122177},
  blog={https://www.linkedin.com/feed/update/urn:li:activity:7075389038043533312?utm_source=share&utm_medium=member_desktop},
  selected={true}
}


 
 


@article{van2023semiparametric,
  title={Semiparametric logistic regression for inference on relative vaccine
efficacy in case-only studies with informative missingness},
  author={{van der Laan}, Lars and Gilbert, Peter B},
  journal={arXiv preprint arXiv:2303.11462},
  year={2023},
  keywords={methods},
  pdf={https://arxiv.org/pdf/2303.11462},
  abstract={
  We develop semiparametric methods for estimating subgroup-specific relative vaccine efficacy against
multiple viral strains in a partially vaccinated population. Focusing on observational case-only studies,
we address informative missingness in strain type due to vaccination status, pre-vaccination character-
istics, and post-infection factors such as viral load. We establish general conditions for the nonpara-
metric identification of relative conditional vaccine efficacy between strains using covariate-adjusted
conditional odds ratio parameters. Assuming a log-linear parametric form for strain-specific conditional
vaccine efficacy, we propose targeted maximum likelihood estimators based on partially linear logis-
tic regression, leveraging machine learning for flexible confounding adjustment. Finally, we apply our
methods to estimate relative strain-specific conditional vaccine efficacy in the ENSEMBLE COVID-19
vaccine trial.}
}


@article{van2022nonparametric,
  title={Nonparametric estimation of the causal effect of a stochastic threshold-based intervention},
  author={{van der Laan}, Lars and Zhang, Wenbo and Gilbert, Peter B},
  journal={Biometrics},
  year={2022},
  keywords={methods},
  pdf={https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13690},
  abstract={Identifying a biomarker or treatment-dose threshold that marks a specified level of risk is an important problem, especially in clinical trials. In view of this goal, we consider a covariate-adjusted threshold-based interventional estimand, which happens to equal the binary treatment–specific mean estimand from the causal inference literature obtained by dichotomizing the continuous biomarker or treatment as above or below a threshold. The unadjusted version of this estimand was considered in Donovan et al.. Expanding upon Stitelman et al., we show that this estimand, under conditions, identifies the expected outcome of a stochastic intervention that sets the treatment dose of all participants above the threshold. We propose a novel nonparametric efficient estimator for the covariate-adjusted threshold-response function for the case of informative outcome missingness, which utilizes machine learning and targeted minimum-loss estimation (TMLE). We prove the estimator is efficient and characterize its asymptotic distribution and robustness properties. Construction of simultaneous 95% confidence bands for the threshold-specific estimand across a set of thresholds is discussed. In the Supporting Information, we discuss how to adjust our estimator when the biomarker is missing at random, as occurs in clinical trials with biased sampling designs, using inverse probability weighting. Efficiency and bias reduction of the proposed estimator are assessed in simulations. The methods are employed to estimate neutralizing antibody thresholds for virologically confirmed dengue risk in the CYD14 and CYD15 dengue vaccine trials.}
}



@article{wang2025targeted,
  title={Targeted maximum likelihood based estimation for longitudinal mediation analysis},
  author={Wang, Zeyi and {van der Laan}, Lars and Petersen, Maya and Gerds, Thomas and Kvist, Kajsa and {van der Laan}, Mark},
  journal={Journal of Causal Inference},
  volume={13},
  number={1},
  pages={20230013},
  year={2025},
  keywords={methods},
  publisher={De Gruyter},
  pdf={https://arxiv.org/pdf/2304.04904}
}


 

@article{van2021higher,
  title={Higher order targeted maximum likelihood estimation},
  author={{van der Laan}, Mark and Wang, Zeyi and {van der Laan}, Lars},
  journal={arXiv preprint arXiv:2101.06290},
  keywords={effest},
  year={2021},
  pdf={https://arxiv.org/pdf/2101.06290}
}

@software{coyle2022hal9001-rpkg,
      author = {Coyle, Jeremy R and Hejazi, Nima S and Phillips, Rachael V
        and {van der Laan}, Lars and {van der Laan}, Mark J},
      title = {{hal9001}: The scalable highly adaptive lasso},
      year  = {2022},
      keywords={software},
      url = {https://tlverse.org/hal9001/},
      pdf={https://www.theoj.org/joss-papers/joss.02526/10.21105.joss.02526.pdf},
      note = {{R} package version 0.4.2},
      selected={false}
    }


 
 
